#!/bin/bash

echo "Welcome to Malkon's Ollama installer !"
echo "I don't host the models listed, this is just to simplify the installation of a local AI !"
echo "Also credits to everyone who made the models and who made Ollama !"
echo "Published under the GPL license !"

sleep 0.25

detect_package_manager() {
    if command -v apt &> /dev/null; then
        echo "apt"
    elif command -v dnf &> /dev/null; then
        echo "dnf"
    elif command -v yum &> /dev/null; then
        echo "yum"
    elif command -v pacman &> /dev/null; then
        echo "pacman"
    elif command -v zypper &> /dev/null; then
        echo "zypper"
    else
        echo "unknown"
    fi
}

PM=$(detect_package_manager)

if ! command -v ollama &> /dev/null || ! ollama --version &> /dev/null; then
    echo "Oh noes ! Ollama is not installed or not functioning properly, but we'll fix it !"
    echo "Installing Ollama for you... :-P"

    if ! command -v curl &> /dev/null; then
        echo "curl is not installed. Installing curl using $PM... :-P"
        case "$PM" in
            apt)
                sudo apt update && sudo apt install -y curl
                ;;
            dnf)
                sudo dnf install -y curl
                ;;
            yum)
                sudo yum install -y curl
                ;;
            pacman)
                sudo pacman -Sy --noconfirm curl
                ;;
            zypper)
                sudo zypper install -y curl
                ;;
            *)
                echo "Oh noes, it seems like you're running this installer in an unsupported environment ! Please manually install curl to proceed or wait for official support for this environment !"
                exit 1
                ;;
        esac
    fi

    curl -fsSL https://ollama.com/install.sh | sh

    echo "Ollama installed successfully! Please restart the terminal if needed, or your system may not detect Ollama !"
 exit 0
else
 echo "Ollama is already installed and working !"
fi


sleep 0.25

echo "Choose a model to install:"
echo
echo " #   Model                    Size     Purpose"
echo "-------------------------------------------------------------"
echo " 1)  Phi 4                    9.1GB    Smart. Fast. For general use. Requires reasonably beefy computer, though."
echo
echo " 2)  Gemma 3                  815MB    Recommended for most users who just want stuff that works"
echo
echo " 3)  Codellama                3.8GB    Dedicated coding assistant. You can probably tell by the name."
echo
echo " 4)  Llama 2 Uncensored       3.8GB    Less censored general use AI... I wonder why you would need this."
echo
echo " 5)  Phi 4 Mini               2.5GB    Phi 4 except it's recommendable for an average user"
echo
echo " 6)  I'm not sure...          0GB      You're just unsure, we don't blame you. This is for recommendations!"
echo
echo

read -p "Enter the number of the model you want to install [1-6]: " choice

case $choice in
    1)
        model="phi4"
        ;;
    2)
        model="gemma3:1b"
        ;;
    3)
        model="codellama"
        ;;
    4)
        model="llama2-uncensored"
        ;;
    5)
        model="phi4-mini"
        ;;
    6)
        echo "Hi! I'll help you out for this!"
echo
        echo "1. Are you just chilling on your laptop, or any other personal device? You should use Phi 4 Mini or Gemma 3! Phi 4 Mini requires a slightly better device in terms of performance, while Gemma 3 is your go-to for average use!"
echo
        echo "2. Maybe you have a beefy computer or device? Something dedicated for pure performance? Possibly something you built and knew what you did with? Phi 4 is your go!"
echo
        echo "3. Maybe you have a server? Multiple Graphics Processing Units? Amounts of RAM you didn't know existed? Phi 4 is pretty good, but you're probably gonna be liking Codellama for this!"
echo
        echo "We will add more models later, as for now this is just a test and is WIP! So don't feel down if your prefered model is not on here, there are probably tons of tutorials showing you how to install your prefered model !"
echo
        echo "You don't have to worry about installing Ollama, this script likely already did that for you !"
echo
exit 0
        ;;
    *)
        echo "Pick something that's listed, silly !"
        exit 1
        ;;
esac

echo "You selected: $model"
echo "Starting Ollama... :-P"
ollama serve
echo "Pulling model from Ollama... :-P"
ollama pull "$model"

cat <<EOF > ~/AI.sh
#!/bin/bash
# This script was generated by Malkon's local AI installer script !
# Licensed under GPL v3
# Feel free to modify the script to your needs !
ollama run $model # CRITICAL LINE !!! Do not remove unless you know what you are doing >:O
EOF

chmod +x ~/AI.sh
echo "Created AI.sh in the home directory (~/AI.sh) and made it executable successfully"

echo
echo "Done! You can now run your AI by typing: ~/AI.sh"


read -p "Do you want to launch the AI now? [Y/N]: " choice2

case "${choice2,,}" in
     y|yes)
        ~/AI.sh
        ;;

     n|no)
       exit 0
       ;;
     *)
       echo "Oh well... we didn't quite get that!"
       echo "If you want to launch it, type '~/AI.sh' !"
esac
